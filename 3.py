# -*- coding: utf-8 -*-
"""3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vlyXOO-ps4lLj2APN8o6oG4b3anFz4G5
"""

## Import libraries
import numpy as np
from matplotlib import pyplot as plt
from tensorflow import keras
from keras.datasets import mnist
from tensorflow.keras import layers
from keras.layers import Dense, Conv2D, Flatten

# Load test and train data
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

# Understanding test and train data
print(type(X_train))
print(type(Y_train))
print(type(X_test))
print(type(Y_test))

print(X_train.shape) 
print(Y_train.shape) 
print(X_test.shape) 
print(Y_test.shape)

X_train = np.expand_dims(X_train, -1)
X_test = np.expand_dims(X_test, -1)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# check the min and max values
np.min(X_train), np.max(X_train)
#np.min(Y_train), np.max(Y_test)

# Normalize the data
X_train /= 255
X_test /= 255

# Check min and max values
np.min(X_train), np.max(X_train)

# make y data categorical
no_of_classes = 10
Y_train = keras.utils.to_categorical(Y_train, no_of_classes)
Y_test = keras.utils.to_categorical(Y_test, no_of_classes)

# Train the model from section 1 with 0.25 noise factor
# Model1
model1 = keras.Sequential()

# Add Layers
model1.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model1.add(layers.MaxPooling2D((2, 2)))
model1.add(layers.Conv2D(64, (3, 3), activation='relu'))
model1.add(layers.MaxPooling2D((2, 2)))
model1.add(Flatten())
model1.add(Dense(10, activation='softmax'))

# Model1 summary
model1.summary()

# Compile the model1
model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Add noise
noise_factor = 0.25
X_train_noisy_25 = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)
X_test_noisy_25 = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)
X_train_noisy_25 = np.clip(X_train_noisy_25, 0. , 1.)
X_test_noisy_25 = np.clip(X_test_noisy_25, 0. , 1.)

# Fit the model1 with 
model1.fit(X_train_noisy_25, Y_train, batch_size=128, epochs=15, validation_split=0.1)

# Evaluate the model1
score = model1.evaluate(X_test_noisy_25, Y_test, verbose=0)
print("Model accuracy: ", score[1])

# Train the model from section 1 with 0.25 noise factor. Add 1 more layer
# Model2
model2 = keras.Sequential()

# Add Layers
model2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model2.add(layers.MaxPooling2D((2, 2)))
model2.add(layers.Conv2D(64, (3, 3), activation='relu'))
model2.add(layers.MaxPooling2D((2, 2)))
model2.add(layers.Conv2D(64, (3, 3), activation='relu'))
model2.add(layers.MaxPooling2D((2, 2)))
model2.add(Flatten())
model2.add(Dense(10, activation='softmax'))

# Model2 summary
model2.summary()

# Compile the model2
model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Add noise
noise_factor = 0.25
X_train_noisy_25 = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)
X_test_noisy_25 = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)
X_train_noisy_25 = np.clip(X_train_noisy_25, 0. , 1.)
X_test_noisy_25 = np.clip(X_test_noisy_25, 0. , 1.)

# Fit the model2 with 
model2.fit(X_train_noisy_25, Y_train, batch_size=128, epochs=15, validation_split=0.1)

# Evaluate the model2
score = model2.evaluate(X_test_noisy_25, Y_test, verbose=0)
print("Model accuracy: ", score[1])

# Train the model from section 1 with 0.25 noise factor. Add 2 layers
# Model1
model3 = keras.Sequential()

# Add Layers
model3.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model3.add(layers.MaxPooling2D((2, 2)))
model3.add(layers.Conv2D(64, (3, 3), activation='relu'))
model3.add(layers.MaxPooling2D((2, 2)))
model3.add(layers.Conv2D(64, (3, 3), activation='relu'))
model3.add(layers.MaxPooling2D((2, 2)))
model3.add(Flatten())
model3.add(Dense(20, activation='softmax'))
model3.add(Dense(10, activation='softmax'))

# Model3 summary
model3.summary()

# Compile the model3
model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Add noise
noise_factor = 0.25
X_train_noisy_25 = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)
X_test_noisy_25 = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)
X_train_noisy_25 = np.clip(X_train_noisy_25, 0. , 1.)
X_test_noisy_25 = np.clip(X_test_noisy_25, 0. , 1.)

# Fit the model3 with 
model3.fit(X_train_noisy_25, Y_train, batch_size=128, epochs=15, validation_split=0.1)

# Evaluate the model3
score = model3.evaluate(X_test_noisy_25, Y_test, verbose=0)
print("Model accuracy: ", score[1])

# Train the model from section 1 with 0.25 noise factor. Add one conv layer and one dense layers
# Model4
model4 = keras.Sequential()

# Add Layers
model4.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model4.add(layers.MaxPooling2D((2, 2)))
model4.add(layers.Conv2D(64, (3, 3), activation='relu'))
model4.add(layers.MaxPooling2D((2, 2)))
model4.add(Flatten())
model4.add(Dense(20, activation='softmax'))
model4.add(Dense(10, activation='softmax'))

# Model4 summary
model4.summary()

# Compile the model4
model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Add noise
noise_factor = 0.25
X_train_noisy_25 = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)
X_test_noisy_25 = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)
X_train_noisy_25 = np.clip(X_train_noisy_25, 0. , 1.)
X_test_noisy_25 = np.clip(X_test_noisy_25, 0. , 1.)

# Fit the model4 with 
model4.fit(X_train_noisy_25, Y_train, batch_size=128, epochs=15, validation_split=0.1)

# Evaluate the model4
score = model4.evaluate(X_test_noisy_25, Y_test, verbose=0)
print("Model accuracy: ", score[1])

# Train the model from section 1 with 0.25 noise factor. Add one conv layer and one dense layers
# Model5
model5 = keras.Sequential()

# Add Layers
model5.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model5.add(layers.MaxPooling2D((2, 2)))
model5.add(layers.Conv2D(16, (3, 3), activation='relu'))
model5.add(layers.MaxPooling2D((2, 2)))
model5.add(layers.Conv2D(16, (3, 3), activation='relu'))
model5.add(layers.MaxPooling2D((2, 2)))
model5.add(Flatten())
model5.add(Dense(10, activation='softmax'))
model5.add(Dense(10, activation='softmax'))

# Model5 summary
model5.summary()

# Compile the model5
model5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Add noise
noise_factor = 0.25
X_train_noisy_25 = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)
X_test_noisy_25 = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)
X_train_noisy_25 = np.clip(X_train_noisy_25, 0. , 1.)
X_test_noisy_25 = np.clip(X_test_noisy_25, 0. , 1.)

# Fit the model5 with 
model5.fit(X_train_noisy_25, Y_train, batch_size=128, epochs=15, validation_split=0.1)

# Evaluate the model5
score = model5.evaluate(X_test_noisy_25, Y_test, verbose=0)
print("Model accuracy: ", score[1])

# Train the model from section 1 with 0.25 noise factor. Add one conv layer and one dense layers
# Model6
model6 = keras.Sequential()

# Add Layers
model6.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1)))
model6.add(layers.MaxPooling2D((2, 2)))
model6.add(layers.Conv2D(32, (3, 3), activation='relu'))
model6.add(layers.MaxPooling2D((2, 2)))
model6.add(layers.Conv2D(64, (3, 3), activation='relu'))
model6.add(layers.MaxPooling2D((2, 2)))
model6.add(Flatten())
model6.add(Dense(10, activation='softmax'))
model6.add(Dense(20, activation='softmax'))
model6.add(Dense(10, activation='softmax'))

# Model6 summary
model6.summary()

# Compile the model6
model6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Add noise
noise_factor = 0.25
X_train_noisy_25 = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)
X_test_noisy_25 = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)
X_train_noisy_25 = np.clip(X_train_noisy_25, 0. , 1.)
X_test_noisy_25 = np.clip(X_test_noisy_25, 0. , 1.)

# Fit the model6 with 
model6.fit(X_train_noisy_25, Y_train, batch_size=128, epochs=15, validation_split=0.1)

# Evaluate the model6
score = model6.evaluate(X_test_noisy_25, Y_test, verbose=0)
print("Model accuracy: ", score[1])